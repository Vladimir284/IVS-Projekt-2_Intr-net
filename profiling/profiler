In the attached graphs, we can see that we tested 1000, 1000 000 and 10 000 000 numbers.
As you can see at the first graph, when we used only 1000 numbers, the profiler output doesn't even show us the time spent on calling particular functions.
So this output doesn't really tell us much.

But from the other two graphs it's visible that the functions, which took the longest time and were called the most are the functions, that you repeatedly need to calculate the standard deviation.
Those functions are: summary, subtraction and nth-power.

Those functions maybe were called the most and the profiler spent there the most of the time, but the time spent there was still pretty short.

To sum it up, I think that our functions summary and subtraction are really fast and they don't need any optimalization.

For calculation of the nth-power of the number we used the python build-in function (**), which seems pretty good optimalized too. Honestly I don't think we could implement it any better.
